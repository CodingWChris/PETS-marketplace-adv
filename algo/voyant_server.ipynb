{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import json\n",
    "import urllib.parse\n",
    "import webbrowser\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    \"\"\"Clean and tokenize text\"\"\"\n",
    "    # Remove HTML tags\n",
    "    clean = re.compile('<.*?>')\n",
    "    text = re.sub(clean, '', text)\n",
    "    \n",
    "    # Split into sentences for trend analysis\n",
    "    sentences = sent_tokenize(text)\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word.isalnum() and word not in stopwords]\n",
    "    \n",
    "    return tokens, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_word_trends(sentences, words):\n",
    "    \"\"\"Calculate word frequency trends across document segments\"\"\"\n",
    "    trends = {word: [] for word in words}\n",
    "    chunk_size = max(1, len(sentences) // 10)  # Split into 10 segments\n",
    "    \n",
    "    for i in range(0, len(sentences), chunk_size):\n",
    "        chunk = ' '.join(sentences[i:i+chunk_size])\n",
    "        chunk_tokens = word_tokenize(chunk.lower())\n",
    "        chunk_freq = Counter(chunk_tokens)\n",
    "        \n",
    "        for word in words:\n",
    "            trends[word].append(chunk_freq[word])\n",
    "    \n",
    "    return trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collocations(tokens, window_size=5):\n",
    "    \"\"\"Find word collocations\"\"\"\n",
    "    collocations = []\n",
    "    for i in range(len(tokens) - window_size):\n",
    "        window = tokens[i:i+window_size]\n",
    "        for j in range(len(window)):\n",
    "            for k in range(j+1, len(window)):\n",
    "                collocations.append((window[j], window[k]))\n",
    "    return Counter(collocations).most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text(text):\n",
    "    \"\"\"Analyze text and generate visualizations\"\"\"\n",
    "    # Process text\n",
    "    tokens, sentences = process_text(text)\n",
    "    \n",
    "    # Word frequency\n",
    "    word_freq = Counter(tokens)\n",
    "    top_words = dict(word_freq.most_common(100))\n",
    "    \n",
    "    # Generate visualizations\n",
    "    fig_freq = px.bar(x=list(top_words.keys())[:20], \n",
    "                      y=list(top_words.values())[:20],\n",
    "                      title='Top 20 Word Frequencies')\n",
    "    fig_freq.show()\n",
    "    \n",
    "    # Word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400).generate(' '.join(tokens))\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Trend analysis\n",
    "    trends = calculate_word_trends(sentences, list(top_words.keys())[:10])\n",
    "    fig_trends = go.Figure()\n",
    "    for word, counts in trends.items():\n",
    "        fig_trends.add_trace(go.Scatter(y=counts, name=word, mode='lines+markers'))\n",
    "    fig_trends.update_layout(title='Word Frequency Trends')\n",
    "    fig_trends.show()\n",
    "    \n",
    "    return {\n",
    "        'word_frequencies': top_words,\n",
    "        'total_words': len(tokens),\n",
    "        'unique_words': len(set(tokens))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_to_voyant(text):\n",
    "    \"\"\"Send text to local Voyant server\"\"\"\n",
    "    encoded_text = urllib.parse.quote(text)\n",
    "    voyant_url = f'http://localhost:8888/?input={encoded_text}'\n",
    "    webbrowser.open(voyant_url)\n",
    "    return \"Opened in Voyant Tools\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "file_path = '../data/inputs/enron/enron_subset.csv'\n",
    "\n",
    "# Read the file\n",
    "if file_path.endswith('.csv'):\n",
    "    df = pd.read_csv(file_path)\n",
    "    text = ' '.join(df['message'].dropna().tolist())\n",
    "else:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "# Option 1: Analyze locally\n",
    "results = analyze_text(text)\n",
    "print(f\"Total words: {results['total_words']}\")\n",
    "print(f\"Unique words: {results['unique_words']}\")\n",
    "\n",
    "# Option 2: Send to Voyant\n",
    "send_to_voyant(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}