{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/itschris/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/itschris/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import json\n",
    "import urllib.parse\n",
    "import webbrowser\n",
    "from email.parser import Parser\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    \"\"\"Clean and tokenize text\"\"\"\n",
    "    # Remove HTML tags\n",
    "    clean = re.compile('<.*?>')\n",
    "    text = re.sub(clean, '', text)\n",
    "\n",
    "    # only keep the email body information \n",
    "    email = Parser().parsestr(text)\n",
    "    email_body= email.get_payload()\n",
    "\n",
    "    # Split into words and sentences\n",
    "    sen_tokens = sent_tokenize(email_body)\n",
    "    wrd_tokens = word_tokenize(email_body.lower())\n",
    "\n",
    "    # Remove stopwords & punctuation\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    filtered_wrds_token = [word for word in wrd_tokens if word.isalnum() and word not in stopwords and word not in string.punctuation]\n",
    "    \n",
    "    return filtered_wrds_token, sen_tokens, ' '.join(filtered_wrds_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_word_trends(sentences, words):\n",
    "    \"\"\"Calculate word frequency trends across document segments\"\"\"\n",
    "    trends = {word: [] for word in words}\n",
    "    chunk_size = max(1, len(sentences) // 10)  # Split into 10 segments\n",
    "    \n",
    "    for i in range(0, len(sentences), chunk_size):\n",
    "        chunk = ' '.join(sentences[i:i+chunk_size])\n",
    "        chunk_tokens = word_tokenize(chunk.lower())\n",
    "        chunk_freq = Counter(chunk_tokens)\n",
    "        \n",
    "        for word in words:\n",
    "            trends[word].append(chunk_freq[word])\n",
    "    \n",
    "    return trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in a specified window size, find the top 50 most common collocations in the data \n",
    "\n",
    "def get_collocations(tokens, window_size=5):\n",
    "    \"\"\"Find word collocations\"\"\"\n",
    "    collocations = []\n",
    "    for i in range(len(tokens) - window_size):\n",
    "        window = tokens[i:i+window_size]\n",
    "        for j in range(len(window)):\n",
    "            for k in range(j+1, len(window)):\n",
    "                collocations.append((window[j], window[k]))\n",
    "    return Counter(collocations).most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text(text):\n",
    "    \"\"\"Analyze text and generate visualizations\"\"\"\n",
    "    # Process text\n",
    "    tokens, sentences = preprocessing(text)\n",
    "    \n",
    "    # Word frequency\n",
    "    word_freq = Counter(tokens)\n",
    "    top_words = dict(word_freq.most_common(100))\n",
    "    \n",
    "    # Generate visualizations\n",
    "    fig_freq = px.bar(x=list(top_words.keys())[:20], \n",
    "                      y=list(top_words.values())[:20],\n",
    "                      title='Top 20 Word Frequencies')\n",
    "    fig_freq.show()\n",
    "    \n",
    "    # Word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400).generate(' '.join(tokens))\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Trend analysis\n",
    "    trends = calculate_word_trends(sentences, list(top_words.keys())[:10])\n",
    "    fig_trends = go.Figure()\n",
    "    for word, counts in trends.items():\n",
    "        fig_trends.add_trace(go.Scatter(y=counts, name=word, mode='lines+markers'))\n",
    "    fig_trends.update_layout(title='Word Frequency Trends')\n",
    "    fig_trends.show()\n",
    "    \n",
    "    return {\n",
    "        'word_frequencies': top_words,\n",
    "        'total_words': len(tokens),\n",
    "        'unique_words': len(set(tokens))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_to_voyant(text):\n",
    "    \"\"\"Send text to local Voyant server\"\"\"\n",
    "    encoded_text = urllib.parse.quote(text)\n",
    "    voyant_url = f'http://localhost:8888/?input={encoded_text}'\n",
    "    webbrowser.open(voyant_url)\n",
    "    return \"Opened in Voyant Tools\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_to_voyant_large_text(text, return_url=False):\n",
    "    \"\"\"Create a Voyant Tools corpus and return/open the URL\"\"\"\n",
    "    # Use the public Voyant Tools server\n",
    "    base_url = \"https://voyant-tools.org/upload\"\n",
    "    \n",
    "    # Create a temporary file with the text\n",
    "    with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:\n",
    "        f.write(text)\n",
    "        temp_filename = f.name\n",
    "    \n",
    "    # Create multipart form data\n",
    "    files = {\n",
    "        'upload': (os.path.basename(temp_filename), open(temp_filename, 'rb'), 'text/plain')\n",
    "    }\n",
    "    \n",
    "    # Upload the file\n",
    "    response = requests.post(base_url, files=files)\n",
    "    \n",
    "    # Clean up temporary file\n",
    "    os.unlink(temp_filename)\n",
    "    \n",
    "    # Get the corpus ID from the response\n",
    "    if response.status_code == 200:\n",
    "        corpus_url = f\"https://voyant-tools.org/?corpus={response.text.strip()}\"\n",
    "        \n",
    "        if return_url:\n",
    "            return corpus_url\n",
    "        else:\n",
    "            webbrowser.open(corpus_url)\n",
    "            return \"Opened in Voyant Tools\"\n",
    "    else:\n",
    "        return f\"Error: {response.status_code}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_to_official_voyant(text, return_url=False):\n",
    "    \"\"\"Send text to Voyant Tools and return/open the URL\"\"\"\n",
    "    \n",
    "    # Use the public Voyant Tools server with the correct endpoint\n",
    "    base_url = \"https://voyant-tools.org/\"\n",
    "    \n",
    "    # Create form data\n",
    "    data = {\n",
    "        'inputFormat': 'text',\n",
    "        'input': text\n",
    "    }\n",
    "    \n",
    "    # Send POST request\n",
    "    try:\n",
    "        response = requests.post(base_url, data=data)\n",
    "        if response.status_code == 200:\n",
    "            # Extract the corpus ID from the response URL\n",
    "            corpus_url = response.url\n",
    "            \n",
    "            if return_url:\n",
    "                return corpus_url\n",
    "            else:\n",
    "                webbrowser.open(corpus_url)\n",
    "                return \"Opened in Voyant Tools\"\n",
    "        else:\n",
    "            return f\"Error: Server returned status code {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enron email dataset\n",
    "file_path = '../data/inputs/enron/enron_subset.csv'\n",
    "\n",
    "# Read the file\n",
    "if file_path.endswith('.csv'):\n",
    "    df = pd.read_csv(file_path)\n",
    "    text = ' '.join(df['message'].dropna().tolist())\n",
    "else:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Analyze locally\n",
    "# results = analyze_text(text)\n",
    "# print(f\"Total words: {results['total_words']}\")\n",
    "# print(f\"Unique words: {results['unique_words']}\")\n",
    "\n",
    "# Option 2: Send to Voyant\n",
    "words, sentence, email_body = preprocessing(text)\n",
    "\n",
    "url = send_to_official_voyant(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://voyant-tools.org/?corpus=f7087499a373975c14fc89c568b3dbe0'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
